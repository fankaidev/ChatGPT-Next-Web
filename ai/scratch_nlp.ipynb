{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fankaidev/ChatGPT-Next-Web/blob/main/ai/scratch_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XSmwIR97dsMG"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import os\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('.', ' .')\n",
        "    words = text.split(' ')\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "    '''コサイン類似度の算出\n",
        "\n",
        "    :param x: ベクトル\n",
        "    :param y: ベクトル\n",
        "    :param eps: ”0割り”防止のための微小値\n",
        "    :return:\n",
        "    '''\n",
        "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
        "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
        "    return np.dot(nx, ny)\n",
        "\n",
        "\n",
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
        "    '''類似単語の検索\n",
        "\n",
        "    :param query: クエリ（テキスト）\n",
        "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
        "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
        "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
        "    :param top: 上位何位まで表示するか\n",
        "    '''\n",
        "    if query not in word_to_id:\n",
        "        print('%s is not found' % query)\n",
        "        return\n",
        "\n",
        "    print('\\n[query] ' + query)\n",
        "    query_id = word_to_id[query]\n",
        "    query_vec = word_matrix[query_id]\n",
        "\n",
        "    vocab_size = len(id_to_word)\n",
        "\n",
        "    similarity = np.zeros(vocab_size)\n",
        "    for i in range(vocab_size):\n",
        "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if id_to_word[i] == query:\n",
        "            continue\n",
        "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def convert_one_hot(corpus, vocab_size):\n",
        "    '''one-hot表現への変換\n",
        "\n",
        "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
        "    :param vocab_size: 語彙数\n",
        "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
        "    '''\n",
        "    N = corpus.shape[0]\n",
        "\n",
        "    if corpus.ndim == 1:\n",
        "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
        "        for idx, word_id in enumerate(corpus):\n",
        "            one_hot[idx, word_id] = 1\n",
        "\n",
        "    elif corpus.ndim == 2:\n",
        "        C = corpus.shape[1]\n",
        "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
        "        for idx_0, word_ids in enumerate(corpus):\n",
        "            for idx_1, word_id in enumerate(word_ids):\n",
        "                one_hot[idx_0, idx_1, word_id] = 1\n",
        "\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
        "    '''共起行列の作成\n",
        "\n",
        "    :param corpus: コーパス（単語IDのリスト）\n",
        "    :param vocab_size:語彙数\n",
        "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
        "    :return: 共起行列\n",
        "    '''\n",
        "    corpus_size = len(corpus)\n",
        "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        for i in range(1, window_size + 1):\n",
        "            left_idx = idx - i\n",
        "            right_idx = idx + i\n",
        "\n",
        "            if left_idx >= 0:\n",
        "                left_word_id = corpus[left_idx]\n",
        "                co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "            if right_idx < corpus_size:\n",
        "                right_word_id = corpus[right_idx]\n",
        "                co_matrix[word_id, right_word_id] += 1\n",
        "\n",
        "    return co_matrix\n",
        "\n",
        "\n",
        "def ppmi(C, verbose=False, eps = 1e-8):\n",
        "    '''PPMI（正の相互情報量）の作成\n",
        "\n",
        "    :param C: 共起行列\n",
        "    :param verbose: 進行状況を出力するかどうか\n",
        "    :return:\n",
        "    '''\n",
        "    M = np.zeros_like(C, dtype=np.float32)\n",
        "    N = np.sum(C)\n",
        "    S = np.sum(C, axis=0)\n",
        "    total = C.shape[0] * C.shape[1]\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(C.shape[0]):\n",
        "        for j in range(C.shape[1]):\n",
        "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
        "            M[i, j] = max(0, pmi)\n",
        "\n",
        "            if verbose:\n",
        "                cnt += 1\n",
        "                if cnt % (total//100 + 1) == 0:\n",
        "                    print('%.1f%% done' % (100*cnt/total))\n",
        "    return M\n",
        "\n",
        "\n",
        "def create_contexts_target(corpus, window_size=1):\n",
        "    '''コンテキストとターゲットの作成\n",
        "\n",
        "    :param corpus: コーパス（単語IDのリスト）\n",
        "    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
        "    :return:\n",
        "    '''\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)\n",
        "\n",
        "\n",
        "def to_cpu(x):\n",
        "    import numpy\n",
        "    if type(x) == numpy.ndarray:\n",
        "        return x\n",
        "    return np.asnumpy(x)\n",
        "\n",
        "\n",
        "def to_gpu(x):\n",
        "    import cupy\n",
        "    if type(x) == cupy.ndarray:\n",
        "        return x\n",
        "    return cupy.asarray(x)\n",
        "\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "    total_norm = 0\n",
        "    for grad in grads:\n",
        "        total_norm += np.sum(grad ** 2)\n",
        "    total_norm = np.sqrt(total_norm)\n",
        "\n",
        "    rate = max_norm / (total_norm + 1e-6)\n",
        "    if rate < 1:\n",
        "        for grad in grads:\n",
        "            grad *= rate\n",
        "\n",
        "\n",
        "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
        "    print('evaluating perplexity ...')\n",
        "    corpus_size = len(corpus)\n",
        "    total_loss = 0\n",
        "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
        "    jump = (corpus_size - 1) // batch_size\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
        "        time_offset = iters * time_size\n",
        "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
        "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
        "\n",
        "        try:\n",
        "            loss = model.forward(xs, ts, train_flg=False)\n",
        "        except TypeError:\n",
        "            loss = model.forward(xs, ts)\n",
        "        total_loss += loss\n",
        "\n",
        "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print('')\n",
        "    ppl = np.exp(total_loss / max_iters)\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def eval_seq2seq(model, question, correct, id_to_char,\n",
        "                 verbose=False, is_reverse=False):\n",
        "    correct = correct.flatten()\n",
        "    # 頭の区切り文字\n",
        "    start_id = correct[0]\n",
        "    correct = correct[1:]\n",
        "    guess = model.generate(question, start_id, len(correct))\n",
        "\n",
        "    # 文字列へ変換\n",
        "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
        "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
        "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
        "\n",
        "    if verbose:\n",
        "        if is_reverse:\n",
        "            question = question[::-1]\n",
        "\n",
        "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
        "        print('Q', question)\n",
        "        print('T', correct)\n",
        "\n",
        "        is_windows = os.name == 'nt'\n",
        "\n",
        "        if correct == guess:\n",
        "            mark = colors['ok'] + '☑' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'O'\n",
        "            print(mark + ' ' + guess)\n",
        "        else:\n",
        "            mark = colors['fail'] + '☒' + colors['close']\n",
        "            if is_windows:\n",
        "                mark = 'X'\n",
        "            print(mark + ' ' + guess)\n",
        "        print('---')\n",
        "\n",
        "    return 1 if guess == correct else 0\n",
        "\n",
        "\n",
        "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
        "    for word in (a, b, c):\n",
        "        if word not in word_to_id:\n",
        "            print('%s is not found' % word)\n",
        "            return\n",
        "\n",
        "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
        "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
        "    query_vec = b_vec - a_vec + c_vec\n",
        "    query_vec = normalize(query_vec)\n",
        "\n",
        "    similarity = np.dot(word_matrix, query_vec)\n",
        "\n",
        "    if answer is not None:\n",
        "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
        "\n",
        "    count = 0\n",
        "    for i in (-1 * similarity).argsort():\n",
        "        if np.isnan(similarity[i]):\n",
        "            continue\n",
        "        if id_to_word[i] in (a, b, c):\n",
        "            continue\n",
        "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
        "\n",
        "        count += 1\n",
        "        if count >= top:\n",
        "            return\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    if x.ndim == 2:\n",
        "        s = np.sqrt((x * x).sum(1))\n",
        "        x /= s.reshape((s.shape[0], 1))\n",
        "    elif x.ndim == 1:\n",
        "        s = np.sqrt((x * x).sum())\n",
        "        x /= s\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hZFQ8yU-ejKP"
      },
      "outputs": [],
      "source": [
        "### PTB\n",
        "\n",
        "import urllib.request\n",
        "import pickle\n",
        "\n",
        "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
        "key_file = {\n",
        "    'train':'ptb.train.txt',\n",
        "    'test':'ptb.test.txt',\n",
        "    'valid':'ptb.valid.txt'\n",
        "}\n",
        "save_file = {\n",
        "    'train':'ptb.train.npy',\n",
        "    'test':'ptb.test.npy',\n",
        "    'valid':'ptb.valid.npy'\n",
        "}\n",
        "vocab_file = 'ptb.vocab.pkl'\n",
        "\n",
        "dataset_dir = \"ptb\"\n",
        "if not os.path.exists(dataset_dir):\n",
        "  os.mkdir(dataset_dir)\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print('Downloading ' + file_name + ' ... ')\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    except urllib.error.URLError:\n",
        "        import ssl\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "\n",
        "    print('Done')\n",
        "\n",
        "\n",
        "def load_vocab():\n",
        "    vocab_path = dataset_dir + '/' + vocab_file\n",
        "    print(\"vocab\", vocab_path)\n",
        "\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'rb') as f:\n",
        "            word_to_id, id_to_word = pickle.load(f)\n",
        "        return word_to_id, id_to_word\n",
        "\n",
        "    word_to_id = {}\n",
        "    id_to_word = {}\n",
        "    data_type = 'train'\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in word_to_id:\n",
        "            tmp_id = len(word_to_id)\n",
        "            word_to_id[word] = tmp_id\n",
        "            id_to_word[tmp_id] = word\n",
        "\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump((word_to_id, id_to_word), f)\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "\n",
        "def load_data(data_type='train'):\n",
        "    '''\n",
        "        :param data_type: データの種類：'train' or 'test' or 'valid (val)'\n",
        "        :return:\n",
        "    '''\n",
        "    if data_type == 'val': data_type = 'valid'\n",
        "    save_path = dataset_dir + '/' + save_file[data_type]\n",
        "\n",
        "    word_to_id, id_to_word = load_vocab()\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        corpus = np.load(save_path)\n",
        "        return corpus, word_to_id, id_to_word\n",
        "\n",
        "    file_name = key_file[data_type]\n",
        "    file_path = dataset_dir + '/' + file_name\n",
        "    _download(file_name)\n",
        "\n",
        "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "    np.save(save_path, corpus)\n",
        "    return corpus, word_to_id, id_to_word\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4GXTCXobPbE",
        "outputId": "81fe8f42-f318-461d-e66b-a043f0700040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "covariance matrix\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ]
        }
      ],
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "print(corpus)\n",
        "print(word_to_id)\n",
        "print(id_to_word)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3)  # 小数点后显示3位有效数字\n",
        "print('covariance matrix')\n",
        "print(C)\n",
        "print('-'*50)\n",
        "print('PPMI')\n",
        "print(W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAUjfnFTe5j1",
        "outputId": "668255a5-971d-4dfc-ea1a-7b1e96b2907c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab ptb/ptb.vocab.pkl\n",
            "Downloading ptb.train.txt ... \n",
            "Done\n",
            "counting  co-occurrence ...\n",
            "calculating PPMI ...\n",
            "1.0% done\n",
            "2.0% done\n",
            "3.0% done\n",
            "4.0% done\n",
            "5.0% done\n",
            "6.0% done\n",
            "7.0% done\n",
            "8.0% done\n",
            "9.0% done\n",
            "10.0% done\n",
            "11.0% done\n",
            "12.0% done\n",
            "13.0% done\n",
            "14.0% done\n",
            "15.0% done\n",
            "16.0% done\n",
            "17.0% done\n",
            "18.0% done\n",
            "19.0% done\n",
            "20.0% done\n",
            "21.0% done\n",
            "22.0% done\n",
            "23.0% done\n",
            "24.0% done\n",
            "25.0% done\n",
            "26.0% done\n",
            "27.0% done\n",
            "28.0% done\n",
            "29.0% done\n",
            "30.0% done\n",
            "31.0% done\n",
            "32.0% done\n",
            "33.0% done\n",
            "34.0% done\n",
            "35.0% done\n",
            "36.0% done\n",
            "37.0% done\n",
            "38.0% done\n",
            "39.0% done\n",
            "40.0% done\n",
            "41.0% done\n",
            "42.0% done\n",
            "43.0% done\n",
            "44.0% done\n",
            "45.0% done\n",
            "46.0% done\n",
            "47.0% done\n",
            "48.0% done\n",
            "49.0% done\n",
            "50.0% done\n",
            "51.0% done\n",
            "52.0% done\n",
            "53.0% done\n",
            "54.0% done\n",
            "55.0% done\n",
            "56.0% done\n",
            "57.0% done\n",
            "58.0% done\n",
            "59.0% done\n",
            "60.0% done\n",
            "61.0% done\n",
            "62.0% done\n",
            "63.0% done\n",
            "64.0% done\n",
            "65.0% done\n",
            "66.0% done\n",
            "67.0% done\n",
            "68.0% done\n",
            "69.0% done\n",
            "70.0% done\n",
            "71.0% done\n",
            "72.0% done\n",
            "73.0% done\n",
            "74.0% done\n",
            "75.0% done\n",
            "76.0% done\n",
            "77.0% done\n",
            "78.0% done\n",
            "79.0% done\n",
            "80.0% done\n",
            "81.0% done\n",
            "82.0% done\n",
            "83.0% done\n",
            "84.0% done\n",
            "85.0% done\n",
            "86.0% done\n",
            "87.0% done\n",
            "88.0% done\n",
            "89.0% done\n",
            "90.0% done\n",
            "91.0% done\n",
            "92.0% done\n",
            "93.0% done\n",
            "94.0% done\n",
            "95.0% done\n",
            "96.0% done\n",
            "97.0% done\n",
            "98.0% done\n",
            "99.0% done\n",
            "calculating SVD ...\n",
            "\n",
            "[query] you\n",
            " i: 0.6324194073677063\n",
            " we: 0.614907443523407\n",
            " anybody: 0.5782585740089417\n",
            " else: 0.5606536865234375\n",
            " 'll: 0.5553814172744751\n",
            "\n",
            "[query] year\n",
            " quarter: 0.6641765236854553\n",
            " month: 0.6409639120101929\n",
            " next: 0.6070217490196228\n",
            " third: 0.6037698984146118\n",
            " earlier: 0.600435733795166\n",
            "\n",
            "[query] car\n",
            " luxury: 0.6077849864959717\n",
            " auto: 0.6012241840362549\n",
            " truck: 0.5601891875267029\n",
            " cars: 0.5238932371139526\n",
            " vehicle: 0.49393463134765625\n",
            "\n",
            "[query] toyota\n",
            " motor: 0.7080283761024475\n",
            " motors: 0.6489861011505127\n",
            " nissan: 0.637113094329834\n",
            " mazda: 0.6365540027618408\n",
            " lexus: 0.5962023735046387\n"
          ]
        }
      ],
      "source": [
        "\n",
        "window_size = 2\n",
        "wordvec_size = 100\n",
        "\n",
        "corpus, word_to_id, id_to_word = load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "print('counting  co-occurrence ...')\n",
        "C = create_co_matrix(corpus, vocab_size, window_size)\n",
        "print('calculating PPMI ...')\n",
        "W = ppmi(C, verbose=True)\n",
        "\n",
        "print('calculating SVD ...')\n",
        "try:\n",
        "    # truncated SVD (fast!)\n",
        "    from sklearn.utils.extmath import randomized_svd\n",
        "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
        "                             random_state=None)\n",
        "except ImportError:\n",
        "    # SVD (slow)\n",
        "    U, S, V = np.linalg.svd(W)\n",
        "\n",
        "word_vecs = U[:, :wordvec_size]\n",
        "\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrMiQMwFxAdqcTuxZF/w8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}